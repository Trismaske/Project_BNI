{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter File Name: bcxlocation_20190701_00.parquet\n",
      "Enter Dataset ID(If Big Query file): \n",
      "Enter Project ID(If Big Query file): \n"
     ]
    }
   ],
   "source": [
    "def df_read_file(file,**kwargs):\n",
    "    dataset_id = kwargs.get('dataset_id', None)\n",
    "    project_id = kwargs.get('project_id', None)\n",
    "    if '.parquet' in file:\n",
    "        data = pd.read_parquet(file, engine = 'pyarrow')\n",
    "    elif '.csv' in file:\n",
    "        data = pd.read_csv(file)\n",
    "    elif '.json' in file:\n",
    "        bigquery_client = bigquery.Client(project = project_id)\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = file\n",
    "        QUERY = \"SELECT * FROM \" +  '`' + dataset_id + '`'\n",
    "        job = bigquery_client.query(QUERY)\n",
    "        data = job.to_dataframe()\n",
    "    return data\n",
    "input_file = input('Enter File Name: ')\n",
    "df = df_read_file(input_file,dataset_id = input('Enter Dataset ID(If Big Query file): '),project_id = input('Enter Project ID(If Big Query file): '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Output Type(csv, json, parquet): json\n",
      "Enter Output File Name: Output\n",
      "Enter Output File Path: C:\\Users\\caylin\\Downloads\n"
     ]
    }
   ],
   "source": [
    "def df_export_file(output_type, file_name, file_path):\n",
    "    if 'parquet' in output_type:\n",
    "        output = df.to_parquet(self, file_name, engine='auto', compression='snappy', index=None, partition_cols=None, **kwargs)\n",
    "    elif 'csv' in output_type:\n",
    "        output = df.to_csv(r''+ file_path + '\\\\' + file_name + '.csv')\n",
    "    elif 'json' in output_type:\n",
    "        output = df.to_json(r''+ file_path + '\\\\' + file_name + '.json')\n",
    "    return output\n",
    "\n",
    "output_file_type = input('Enter Output Type(csv, json, parquet): ')\n",
    "output_file_name = input('Enter Output File Name: ')\n",
    "output_file_path = input('Enter Output File Path: ')\n",
    "output_data = df_export_file(output_file_type, output_file_name, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
